{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exampletary Notebook using LLAMA2-chat with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE\n",
    "# to use this repository the fewshot examples must be added to medicaNLP.bronco.fewshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from nervaluate import Evaluator\n",
    "\n",
    "from medicaNLP.bronco.bronco_utils import bronco_llama2_fewshot_prompt, parse_output\n",
    "from medicaNLP.bronco.fewshots import LLAMA2_FEWSHOT_PROMPT\n",
    "from medicaNLP.instructionTuning.evaluate import evaluate_bronco_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "test_data_instruct = load_dataset(\n",
    "    'json',\n",
    "    data_files={'test': \"/path/to/instr_test.json\"}\n",
    ")['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "llm = LLM(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "sampling_params = SamplingParams(temperature=0.1, max_tokens=240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [bronco_llama2_fewshot_prompt(p) for p in test_data_instruct]\n",
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run text generation\n",
    "test_outputs = llm.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect sentences with output from model\n",
    "test_output_sentence = [\n",
    "    {\n",
    "        \"id\": x['id'],\n",
    "        \"text\": x['input'],\n",
    "        \"output\": output.outputs[0].text\n",
    "    } for x, output in zip(test_data_instruct, test_outputs)\n",
    "]\n",
    "len(test_output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse output string and transform to dictionary\n",
    "for pred in pred_data['predictions']:\n",
    "    pred['parsed_output'] = parse_output(pred['output'], ['Medikamente', 'Diagnose', 'Behandlung'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate generated response\n",
    "res, res_per_tag = evaluate_bronco_prediction(\n",
    "    test_path='/data/German-medical-texts/BRONCO150/instruction_data/BRONCO_instr_test.json',\n",
    "    pred_data=pred_data\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
